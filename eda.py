# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BhDghiodtByskU1NAB77YIHqs9P0kFtm
"""

import pandas as pd

# Load the Excel file
players_df = pd.read_excel('Players.xlsx')

# Display the first few rows and some basic information about the dataframe
players_info = players_df.info()
players_head = players_df.head()

players_info, players_head

# Load the CSV file
transfer_values_df = pd.read_csv('Transfer_Values.csv')

# Display the first few rows and some basic information about the dataframe
transfer_values_info = transfer_values_df.info()
transfer_values_head = transfer_values_df.head()

transfer_values_info, transfer_values_head

# Convert Market Value to a numerical format
# Remove '€' and 'm' (for million), then convert to float and multiply by 1 million
transfer_values_df['Market Value'] = transfer_values_df['Market Value'].str.replace('€', '').str.replace('m', '').str.replace('k', '').astype(float)
transfer_values_df['Market Value'] = transfer_values_df['Market Value'].apply(lambda x: x if x > 100 else x * 1000)  # Assuming values < 100 are in 'k'

# Clean and prepare player names for merging
players_df['Player'] = players_df['Player'].str.strip()
transfer_values_df['Name'] = transfer_values_df['Name'].str.strip()

# Convert 'Date of Birth' to datetime and extract year for age comparison
transfer_values_df['Date of Birth'] = pd.to_datetime(transfer_values_df['Date of Birth'].str.extract('(\d{4})')[0], format='%Y')
transfer_values_df['DOB Year'] = transfer_values_df['Date of Birth'].dt.year

# Check the first few rows to ensure changes
transfer_values_df.head(), transfer_values_df['Market Value'].describe()

# initial merge on player names
merged_df = pd.merge(players_df, transfer_values_df, left_on='Player', right_on='Name', how='inner')

# Display the size of the merged dataframe and the first few rows to assess the merge
merged_df_shape = merged_df.shape
merged_head = merged_df.head()

merged_df_shape, merged_head

"""EDA"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style of seaborn
sns.set(style="whitegrid")

# Distribution of Market Values
plt.figure(figsize=(10, 6))
sns.histplot(merged_df['Market Value'], bins=30, kde=True)
plt.title('Distribution of Player Market Values in the English Premier League')
plt.xlabel('Market Value (in thousands of €)')
plt.ylabel('Frequency')
plt.show()

# Summary statistics for Market Value
market_value_summary = merged_df['Market Value'].describe()
market_value_summary

fig, axs = plt.subplots(3, 1, figsize=(10, 18))

# Goals vs. Market Value
sns.scatterplot(x='Gls', y='Market Value', data=merged_df, ax=axs[0])
axs[0].set_title('Goals vs. Market Value')
axs[0].set_xlabel('Goals')
axs[0].set_ylabel('Market Value (in thousands of €)')

# Assists vs. Market Value
sns.scatterplot(x='Ast', y='Market Value', data=merged_df, ax=axs[1])
axs[1].set_title('Assists vs. Market Value')
axs[1].set_xlabel('Assists')
axs[1].set_ylabel('Market Value (in thousands of €)')

# Expected Goals (xG) vs. Market Value
sns.scatterplot(x='xG', y='Market Value', data=merged_df, ax=axs[2])
axs[2].set_title('Expected Goals (xG) vs. Market Value')
axs[2].set_xlabel('Expected Goals (xG)')
axs[2].set_ylabel('Market Value (in thousands of €)')

plt.tight_layout()
plt.show()

# Positional Analysis: Average Market Value by Position
position_market_values = merged_df.groupby('Position')['Market Value'].mean().sort_values(ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x=position_market_values, y=position_market_values.index)
plt.title('Average Market Value by Player Position')
plt.xlabel('Average Market Value (in thousands of €)')
plt.ylabel('Position')
plt.show()

position_market_values

# Age and Market Value Analysis
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Age', y='Market Value', data=merged_df)
plt.title('Age vs. Market Value')
plt.xlabel('Age')
plt.ylabel('Market Value (in thousands of €)')
plt.show()

# Calculating the correlation between age and market value
age_market_value_corr = merged_df[['Age', 'Market Value']].corr().iloc[0,1]
age_market_value_corr

# Segment Analysis of Market Values by Position
plt.figure(figsize=(12, 8))
sns.boxplot(x='Market Value', y='Position', data=merged_df)
plt.title('Market Value Distribution by Position')
plt.xlabel('Market Value (in thousands of €)')
plt.ylabel('Position')
plt.show()

# Preparing for Regression Analysis: Selecting variables for a simple linear regression model
# Focusing on goals (Gls) as a predictor for market value initially
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Selecting the independent variable (X) and the dependent variable (y)
X = merged_df[['Gls']]
y = merged_df['Market Value']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initializing and training the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Making predictions on the testing set
y_pred = model.predict(X_test)

# Calculating the R-squared value to assess the model's performance
r2 = r2_score(y_test, y_pred)
r2

#Correlation Analysis
corr_matrix = merged_df[['Market Value', 'Gls', 'Ast', 'xG', 'Age', 'MP']].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

#Advanced Visualization Techniques
# Example for Pair Plot
sns.pairplot(merged_df[['Market Value', 'Gls', 'Ast', 'xG', 'Age']], diag_kind='kde')
plt.show()

from sklearn.preprocessing import StandardScaler

# Selecting multiple variables for the model
X_multi = merged_df[['Gls', 'Ast', 'xG', 'Age', 'MP']]  # Independent variables

# Standardizing the features
scaler = StandardScaler()
X_multi_scaled = scaler.fit_transform(X_multi)

# Splitting the dataset into training and testing sets for the multiple variables
X_multi_train, X_multi_test, y_train, y_test = train_test_split(X_multi_scaled, y, test_size=0.2, random_state=42)

# Training the multiple linear regression model
model_multi = LinearRegression()
model_multi.fit(X_multi_train, y_train)

# Making predictions on the testing set
y_multi_pred = model_multi.predict(X_multi_test)

# Calculating the R-squared value for the multiple variables model
r2_multi = r2_score(y_test, y_multi_pred)
r2_multi

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Training the Random Forest regressor model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_multi_train, y_train)

# Making predictions on the testing set with the Random Forest model
y_rf_pred = rf_model.predict(X_multi_test)

# Calculating the R-squared value and Mean Squared Error (MSE) for the Random Forest model
r2_rf = r2_score(y_test, y_rf_pred)
mse_rf = mean_squared_error(y_test, y_rf_pred)

r2_rf, mse_rf

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR

# Training the Gradient Boosting regressor model
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_model.fit(X_multi_train, y_train)

# Making predictions on the testing set with the Gradient Boosting model
y_gb_pred = gb_model.predict(X_multi_test)

# Calculating the R-squared value and Mean Squared Error (MSE) for the Gradient Boosting model
r2_gb = r2_score(y_test, y_gb_pred)
mse_gb = mean_squared_error(y_test, y_gb_pred)

# Training the SVM regressor model
svm_model = SVR(kernel='rbf')
svm_model.fit(X_multi_train, y_train)

# Making predictions on the testing set with the SVM model
y_svm_pred = svm_model.predict(X_multi_test)

# Calculating the R-squared value and Mean Squared Error (MSE) for the SVM model
r2_svm = r2_score(y_test, y_svm_pred)
mse_svm = mean_squared_error(y_test, y_svm_pred)

r2_gb, mse_gb, r2_svm, mse_svm

from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline

# Creating interaction terms: Goals per Match (Gls/MP) and Assists per Match (Ast/MP)
merged_df['Gls_per_MP'] = merged_df['Gls'] / merged_df['MP']
merged_df['Ast_per_MP'] = merged_df['Ast'] / merged_df['MP']

# Selecting the features including the new interaction terms
X_advanced = merged_df[['Gls', 'Ast', 'xG', 'Age', 'MP', 'Gls_per_MP', 'Ast_per_MP']]

# Scaling the features
X_advanced_scaled = scaler.fit_transform(X_advanced)

# Applying PCA for dimensionality reduction, retaining 95% of the variance
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_advanced_scaled)

# Splitting the dataset into training and testing sets after PCA
X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Using a pipeline to combine PCA and Random Forest for easier re-use
rf_pca_pipeline = Pipeline([
    ('pca', PCA(n_components=0.95)),
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42))
])

# Training the pipeline on the original scaled features (before applying PCA manually)
X_advanced_train, X_advanced_test, _, _ = train_test_split(X_advanced_scaled, y, test_size=0.2, random_state=42)
rf_pca_pipeline.fit(X_advanced_train, y_train)

# Making predictions with the pipeline
y_pipeline_pred = rf_pca_pipeline.predict(X_advanced_test)

# Calculating the R-squared value for the pipeline
r2_pipeline = r2_score(y_test, y_pipeline_pred)

# Results from PCA dimensionality reduction
pca.n_components_, r2_pipeline

"""Neural Networks"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Define the model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # First hidden layer with input shape
    Dense(64, activation='relu'),  # Second hidden layer
    Dense(1)  # Output layer
])

# Compile the model
model.compile(optimizer=Adam(), loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=2)

# Evaluate the model
test_loss = model.evaluate(X_test, y_test, verbose=2)

print(f"Test Loss: {test_loss}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Defining the neural network model
nn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_advanced_train.shape[1],)),  # First hidden layer with input shape
    Dense(64, activation='relu'),  # Second hidden layer
    Dense(1)  # Output layer
])

# Compiling the model with an optimizer, loss function, and evaluation metric
nn_model.compile(optimizer=Adam(), loss='mean_squared_error')

# Training the model on the training data
history = nn_model.fit(X_advanced_train, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=0)

# Evaluating the model on the test set
loss = nn_model.evaluate(X_advanced_test, y_test, verbose=0)

loss